from model import DualBranchInceptionV3
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import confusion_matrix, precision_score
from torch.utils.tensorboard import SummaryWriter
import os
from dataset import DualModalDataset, FlattenedAugmentedDataset
from tranform import PairedTrainTransform, PairedEvalTransform
from torch.optim.lr_scheduler import ReduceLROnPlateau
from split import stratified_split_auto

# åŸºæœ¬è·¯å¾„å’Œç±»åˆ«å®šä¹‰
base_dir = "data"  # ä¿®æ”¹ä¸ºç›¸å¯¹è·¯å¾„
rgb_root = os.path.join(base_dir, "rgb")
uv_root = os.path.join(base_dir, "uv")
categories = ["stable", "progressive"]
label_map = {cat: idx for idx, cat in enumerate(categories)}

train_pairs, val_pairs, test_pairs, train_labels, val_labels, test_labels = stratified_split_auto(base_dir)
train_samples = [(rgb_path, uv_path, label) for (rgb_path, uv_path), label in zip(train_pairs, train_labels)]
val_samples = [(rgb_path, uv_path, label) for (rgb_path, uv_path), label in zip(val_pairs, val_labels)]

# ğŸ”§ ä¿®å¤1: é™ä½åˆ†è¾¨ç‡ï¼Œå‡å°‘è®¡ç®—é‡
train_transform = PairedTrainTransform(resize_shorter=512, output_size=(512, 512))
val_transform = PairedEvalTransform(resize_shorter=512, output_size=(512, 512))

# Dataset å®ä¾‹åŒ–
train_dataset = DualModalDataset(rgb_root, uv_root, categories, label_map, transform=train_transform)
train_dataset.samples = train_samples
flat_train_dataset = FlattenedAugmentedDataset(train_dataset)

val_dataset = DualModalDataset(rgb_root, uv_root, categories, label_map, transform=val_transform)
val_dataset.samples = val_samples

# ğŸ”§ ä¿®å¤2: å‡å°batch sizeï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
train_loader = DataLoader(flat_train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)

print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(flat_train_dataset)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")

# è®¾å¤‡ä¸æ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DualBranchInceptionV3(
    num_classes=len(categories),
    weight_path=None  # æš‚æ—¶ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œé¿å…è·¯å¾„é—®é¢˜
).to(device)

# ğŸ”§ ä¿®å¤3: ä¼˜åŒ–è®­ç»ƒå‚æ•°
criterion = nn.CrossEntropyLoss(label_smoothing=0.2)  # å¢åŠ label smoothing
optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-3)  # é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ­£åˆ™åŒ–
num_epochs = 100

# ğŸ”§ ä¿®å¤4: ä½¿ç”¨æ›´é€‚åˆçš„å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

# ğŸ”§ ä¿®å¤5: æ·»åŠ æ—©åœæœºåˆ¶
best_val_loss = float('inf')
patience = 15
patience_counter = 0

writer = SummaryWriter(log_dir=os.path.join("runs_fixed", "dual_branch"))

print("å¼€å§‹è®­ç»ƒ...")
for epoch in range(num_epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for rgb_imgs, uv_imgs, labels in train_loader:
        rgb_imgs = rgb_imgs.to(device)
        uv_imgs = uv_imgs.to(device)
        labels = labels.to(device)

        outputs = model(rgb_imgs, uv_imgs)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        
        # ğŸ”§ ä¿®å¤6: æ·»åŠ æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_acc = 100 * correct / total
    train_loss = running_loss / len(train_loader)

    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for rgb_tensor, uv_tensor, label_tensor in val_loader:
            # ğŸ”§ ä¿®å¤7: æ­£ç¡®å¤„ç†éªŒè¯é›†æ•°æ®ç»´åº¦
            if rgb_tensor.dim() == 5:  # [B, 1, C, H, W]
                rgb_tensor = rgb_tensor.squeeze(1)  # [B, C, H, W]
                uv_tensor = uv_tensor.squeeze(1)
            
            rgb_tensor = rgb_tensor.to(device)
            uv_tensor = uv_tensor.to(device)
            label_tensor = label_tensor.to(device)

            outputs = model(rgb_tensor, uv_tensor)
            loss = criterion(outputs, label_tensor)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += label_tensor.size(0)
            val_correct += (predicted == label_tensor).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(label_tensor.cpu().numpy())

    val_acc = 100 * val_correct / val_total
    val_loss /= len(val_loader)

    # è®¡ç®—æŒ‡æ ‡
    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        precision = precision_score(all_labels, all_preds, zero_division=0)
    else:
        specificity = 0
        precision = 0

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)

    # TensorBoardè®°å½•
    writer.add_scalar("Loss/train", train_loss, epoch + 1)
    writer.add_scalar("Loss/val", val_loss, epoch + 1)
    writer.add_scalar("Accuracy/train", train_acc, epoch + 1)
    writer.add_scalar("Accuracy/val", val_acc, epoch + 1)
    writer.add_scalar("Specificity/val", specificity, epoch + 1)
    writer.add_scalar("Precision/val", precision, epoch + 1)
    writer.add_scalar("Learning_Rate", optimizer.param_groups[0]['lr'], epoch + 1)

    # ğŸ”§ ä¿®å¤8: æ”¹è¿›çš„æ¨¡å‹ä¿å­˜ç­–ç•¥å’Œæ—©åœ
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'val_acc': val_acc,
            'specificity': specificity,
            'precision': precision
        }, 'best_model_fixed.pth')
        print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ - Epoch {epoch + 1}, Val Loss: {val_loss:.4f}")
    else:
        patience_counter += 1

    print(f"Epoch [{epoch + 1}/{num_epochs}] "
          f"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% "
          f"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}% "
          f"Specificity: {specificity:.4f} Precision: {precision:.4f} "
          f"LR: {optimizer.param_groups[0]['lr']:.2e}")

    # æ—©åœæ£€æŸ¥
    if patience_counter >= patience:
        print(f"ğŸ›‘ æ—©åœè§¦å‘ - éªŒè¯lossè¿ç»­{patience}ä¸ªepochæœªæ”¹å–„")
        break

writer.close()
print("âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º 'best_model_fixed.pth'")
print("ğŸ“Š ä½¿ç”¨å‘½ä»¤æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir runs_fixed --port=6006")
